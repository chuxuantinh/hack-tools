--撒花）欢迎90src的领导前来视察工作！(撒花--

00000000清明出去骑行，期间暂停更新00000000000

----------------------------------------------
-------------本项目 急需大牛支援--------------
-----------成因->sqlmap检测注入耗时太长-------
--平均一到两分钟才能检测完一个带有参数的链接--
----我不知道应该怎样写初步解决sql注入的规则---
-----------急需老司机帮助!可站内留言----------
----------------邮箱 fiht#qq.com -------------
-----------------感谢您的关注!----------------
-----------------------------------------------

-署名-方大核桃
-邮箱-fiht@qq.com

kkkkkkkkk如果你有什么好的意见或者建议，欢迎在给我提issues，我将十分感激！kkkkkkkkkkkkkk

备注：此爬虫基于python3,外部模块需要安装requests库 pip3 install request <linux>
大致思路：
1.爬取网页上所有链接：按规则分成友情链接，可能存在sql注入的链接，无用的链接（如 /hello/css.css 即为无用链接）
2.顺着友情链接继续，检测每一个友情链接页面上的友链，可能存在sql注入的链接，无用的链接
3.循环往复。将可能存在payload的链接交给sqlmap检测（sqlmapapi）

技术实现难点「新」：
1.url抓取规则，一个路径下只抓取一个？一个域名下只抓取一个？
2.sqlmap注入速度实在让人有点抓狂....是否能模仿safe3先 简单过滤


技术实现难点 「4-1日凌晨更新」：
1.「去除」url去重规则-已解决,但是略微简陋，待时机成熟将会给出我对url去重的解决办法
2.友链爬取的深度-未解决-暂不解决，待正式版本出来前后再解决
3.sqlmap的扫描速度跟不上-暂未解决，可通过分布式以及启动多条sqlmap延缓
4.「新增」数据库方面的问题，存在内存中还是使用sqllite还是mysql以及怎样加入大家共享无效url特征值的模块

使用方法：
不详，此程序正在开发之中，正式版本将于下周日/4.09 开放使用，敬请期待


此爬虫现在已经实现的功能：
给定网站，检测参数，递归查找给定网站的可能存在sql注入的地方（可能存在get型注入的地方）--是全站链接url中存在参数的网页，并非脚本经过检查发现sql注入的网页，检测sql注入的模块已经加入，但现在和用户交互不友好，故未详细说明，预计在本周结束之前将整合各个模块，完成此产品的大部分功能(有人用的话会考虑使用sqlmapapi做分布式扫描)

2016-4-6 23:13 提交了一次：
1.「功能」进一步完善AutoSql和ScanSql，两个脚本通过数据库连接
2.「需求」改善代码质量，精简各个模块，添加注释并通过pep8格式检查
3.「畅想」如果将每个带参数的url都交给sqlmap.速度会比较尴尬...当完善ScanSql和AutoSql与数据库的交互之后开始琢磨自己的扫描规则

2016-4-01 0:06:45 提交了一次：
1.「功能」增加AutoSql.py，开始对接sqlmap
2.「需求」下一步将加入用户交互的模块，python ScanSql 提供单个url扫描和从文本导入，python AutoSql 也将实现上述功能
3.「需求」「非紧急」将检测的特征值做入库（存入数据库）处理
4.「畅想」「后期」大家共享自己检查失败的特征值，提高大家的使用效率，每天从云端检测不可注入字段

2016-3-31 09:17:05 提交了一次：
翘了一节课，实在是罪过
1.「代码」增加了sh.sh文件以及all_webSite文件
2.「功能」粗略实现挂在vps上扫描，晚上只需要去查收漏洞的目标
3.「需求」加入检测post模块的功能(但是get型漏洞都扫不完/sqlmap效率有点低/来场分布式？)「十分紧急」
4.「需求」现在使用方法不是十分简明，完成post检测之后整合各个模块，在程序中给出使用说明书「后期」
5.「需求」sh文件中的sqlmap重定向出来的结果不够友好，等现在vps上扫描完成将用grep来判断是否存在sql注入 「紧急」
6.「需求」再后期，将会去除sh.sh文件，用python实现其功能「非紧急」

2016-03-27 11:01:25 提交了一次：
1.添加了友情链接的去重模块，把之前的self.friend_url 变成了一个全局dic
2.顺着链接下去找的功能 需要爬虫记录已经检测过的url(避免死循环)，由于使用单一数据结构太麻烦，可能会使用数据库（mysql来完成）
3.上次考虑的类的定义问题将变成数据库的设计问题
4.怎样控制爬虫的深度和广度依旧是一个难以解决的问题，暂时的想法是当等待sqlmap确认的url到了一定数目线程就睡眠，但这个显然不科学
5.下一步要做的是设计数据库和重新定义这个脚本的类以及方法

2016-03-26 21:27:25 提交了一次：
1.改善了url的去重算法
2.友链定义存在问题（怎样算友链？同路径带http路径的链接须去除重复值）
3.类的定义问题（上次提交的时候发现对整个扫描体系的把握有点不够，代码十分之凌乱）
4.怎样控制友链的深度和广度

2016-03-25 18:21:15 提交了一次：
1.修复了一些逻辑问题
2.url去重算法有待完善
3.友链需要加入更多的模块（已经扫描？只留下主机名字？）
4.控制友链的深度以及广度

2016-03-25 13:06:20 提交了一次：
目前实现的功能十分简陋
对面向对象的认知不够强大导致代码有点乱
此次提交实现了url参数中payload的简单去重
下一步要做的是使用sqlmapapi实现对目标url的检测

2016-03-24 23：21：20提交了一次：
目前实现的功能还十分简陋
在下一次push前应该完成
1.对友链的规范处理，由于有些友链中依旧存在类似id=XXX此类的payload,所以应该规范，友链应该只保留网站名+路径。
2.去除重复url和重复友情链接
